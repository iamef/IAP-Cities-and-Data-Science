{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.S188/11.S952 Hack the City: Applied Data Science for Public Good\n",
    "\n",
    "# Hacking Workshop 2 : Data Acquisition\n",
    "\n",
    "\n",
    "## Overview\n",
    "In this session, we will guide you to test three major approaches to acquire data: (1) direct downloding, (2) accessing data from the Web, and (3) fetching data via an API. We provide multiple toturials to demonstrate some basic techniques for data acquisition. Since webpages and APIs may vary case by case, we encourage a quick consultation if you are interested in testing a new API. \n",
    "\n",
    "### 1. Access Data from A File\n",
    "\n",
    "#### Workshop Demo: Cambridge Open Data Portal\n",
    "We will go through [Cambridge Open Data Portal](https://www.cambridgema.gov/departments/opendata) in the workshop session.\n",
    "\n",
    "Most city open data portals allow you to directly download data in csv, excel, or shapefile format. For example, you can visit Cambridge Open Data Portal (https://data.cambridgema.gov/browse), select a particular dataset by click \"View Data\" and directly download dataset by clicking \"Export\" and selecting the preferred format (csv, excel, json, shapefile, etc.)\n",
    "\n",
    "Some Tips:\n",
    "\n",
    "- Creating a folder dedicated for raw data (for example, you can name it as \"input\") may help you manage large number of datasets in a project. When a hacking project progresses, you may acquire multiple datasets from various sources and generate some intermediate or output data. So you may want to save them into different folders (for example, input data, processed data, output data) to organize your project.\n",
    "\n",
    "\n",
    "- Pay attention to the file's name when directly downloading a dataset. If there are multiple files with similar naming convention, you may download, read, and concatenate multiple files in a batch. \n",
    "\n",
    "\n",
    "- Normally, you can download a file and save the data in your local drive. However, this approach becomes inefficient when you are working with files in large size. A better way is to directly load a file and read data in Python (see below). Still, this approach takes your local machine's memory to load data. If you are working with extremely large dataset, you may need to consider [load data line by line](https://stackoverflow.com/questions/8009882/how-to-read-a-large-file-line-by-line) or [use Panda with certain techniques](https://towardsdatascience.com/why-and-how-to-use-pandas-with-large-data-9594dda2ea4c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "\n",
    "# To access a file via its url:\n",
    "url = 'https://tidesandcurrents.noaa.gov/sltrends/data/8443970_meantrend.csv'\n",
    "\n",
    "with requests.Session() as s:\n",
    "    download = s.get(url)\n",
    "\n",
    "    decoded_content = download.content.decode('utf-8')\n",
    "    cr = csv.reader(decoded_content.splitlines(), delimiter=',')\n",
    "    \n",
    "    for row in list(cr)[:10]:   #pring the first 10 rows\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the size of your data is managable, you can directly load the csv file as a DataFrame using Pandas:\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://tidesandcurrents.noaa.gov/sltrends/data/8443970_meantrend.csv'\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Access Data from the Web\n",
    "The module __urllib.request__  or __requests__ can be used for downloading data from the web. Typically, retrieving data over HTTP/HTTPS proceeds as the following steps:\n",
    "- Open the web (using URL) with the function __urlopen()__ and obtain the open URL handle. Please note, this is a read-only open file handle, so you need to use __read()__, __readline()__, or __readlines()__ to access actual data.\n",
    "\n",
    "\n",
    "- Parse the data and get the value that you are interested. Parsing techniques may vary depending on web document format (xml, HTML, txt). __BeautifulSoup__ is a popular package used for parsing and accessing HTML documents in Python. We provide a detail example below demostrating how to use urllib.request and BeautifulSoup to get data from HTML web pages. \n",
    "\n",
    "\n",
    "- When you need to get data from multiple web pages (if their URLs are strucutred similarlly), you can programatically (in a for loop, for example) access and parse many pages and save parsed data into a list, dictionary, or dataframe. \n",
    "\n",
    "__NOTE:__ When processing large number of web pages, you may fail to open particular URL. So it is necessary to write an exception to handle such situation.\n",
    "\n",
    "#### Access HTML files\n",
    "HTML is a markup language on the web that is human-readable. You may find the structure of an HTML document familiar, if you have ever written a website. In short, HTML contains text and predefined tags (enclosed in angle brackes <>) that control the presentation of the texts. \n",
    "\n",
    "#### Tutorial: Parsing zipcode level housing affordability index from website. \n",
    "Visit the website https://massachusetts.hometownlocator.com/zip-codes/ and observe how the URL changes when you browsing invidual web page for a particular zipcode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Let's look at this webpage for zipcode 02139:\n",
    "url = \"https://massachusetts.hometownlocator.com/zip-codes/data,zipcode,02139.cfm\"\n",
    "html = urlopen(url)\n",
    "bsObj = BeautifulSoup(html.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print (bsObj)\n",
    "#print (bsObj.body)\n",
    "#print (bsObj.body.td)\n",
    "#print (bsObj.body.td.strong.string)\n",
    "#print (bsObj.body.td.strong.string.split('is '))\n",
    "#print (bsObj.body.td.strong.string.split('is ')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load website and parse zipcode level affordabiltiy index:\n",
    "def get_affordability_index(zipcode):\n",
    "    from bs4 import BeautifulSoup \n",
    "    from urllib.request import urlopen\n",
    "    \n",
    "    url = \"https://massachusetts.hometownlocator.com/zip-codes/data,zipcode,\"+zipcode+\".cfm\"\n",
    "    html = urlopen(url)\n",
    "    bsObj = BeautifulSoup(html.read())\n",
    "    affordability_index = bsObj.body.td.strong.string.split('is ')[-1]\n",
    "    \n",
    "    return int(affordability_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (get_affordability_index('02139'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can run this function with a list of zipcode:\n",
    "for zipcode in ['02139', '02324', '02466', '02642', '02145']:\n",
    "    print (zipcode,':',get_affordability_index(zipcode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access txt files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## This url link is a txt file reporting sea level changes in Boston area.\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "url = \"https://tidesandcurrents.noaa.gov/sltrends/data/8443970_meantrend.txt\"\n",
    "data = urlopen(url) \n",
    "\n",
    "# for line in data: \n",
    "#     print (line.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternatively, you can also use the requests library, which works better in parsing text.\n",
    "import requests\n",
    "response = requests.get(url)\n",
    "\n",
    "#print (response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarlly, if the size of your data is managable, you can directly load the csv file as a DataFrame using Pandas:\n",
    "df = pd.read_csv(url, sep='\\s+') # This txt file uses spaces to split columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fetch Data via API\n",
    "\n",
    "An application programming interface (API) is an interface or communication protocol between different parts of a computer program intended to simplify the implementation and maintenance of software ([Wikipedia](https://en.wikipedia.org/wiki/Application_programming_interface)). There are various APIs provided by public and private sectors that are related to cities, with different specifications and data fetching procedures (typically you can follow a tutorial to get started). \n",
    "\n",
    "Commonly, you need to fetch data by using a GET request, similar to how you request a web page. Then you can specify certain parameters by passing a query string, to get data based on a particular region, time range, or other criteria. Since many APIs only allow a limited number of requests, so it is important for you to define a proper scope with parameters. \n",
    "\n",
    "We provide a tutorial below demonstrating how to fetch data using an Open Street Map (OSM) related API. Please note, all APIs vary by specifications and regulations, so it is necessary to carefully read the manual or tutorial before accessing a new API. Some APIs (for example, Zillow and Twitter) may require an account token as authentication. Normally this can be easily done by registering a developer account and requesting a token. \n",
    "\n",
    "\n",
    "#### Tutorial:  Open Street Map (OSM) Overpass API\n",
    "\n",
    "The Overpass API (formerly known as OSM Server Side Scripting, or OSM3S before 2011) is a read-only API that serves up custom selected parts of the OSM map data. It acts as a database over the web: the client sends a query to the API and gets back the data set that corresponds to the query. https://wiki.openstreetmap.org/wiki/Overpass_API\n",
    "\n",
    "Unlike the OSM main API, which is optimized for editing, Overpass API is optimized for data consumers that need a few elements within a glimpse or up to roughly 10 million elements in some minutes, both selected by search criteria like e.g. location, type of objects, tag properties, proximity, or combinations of them. It acts as a database backend for various services.\n",
    "\n",
    "#### Get existing bus stops from OSM\n",
    "You will identify the map extent as a bounding box for loading OSM data. you can specify a bounding box by (south, west, north, east) in latitude and longitude. Bus stops are nodes in OSM. Inside the query, you can also specify what type of nodes to acquire based on tags.\n",
    "\n",
    "Learn more about how to query data via OverPass API: https://wiki.openstreetmap.org/wiki/Overpass_API/Overpass_QL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Cambirdge, MA\n",
    "overpass_url = \"https://lz4.overpass-api.de/api/interpreter\"\n",
    "\n",
    "overpass_query = \"\"\"\n",
    "[out:json];\n",
    "node[\"highway\"=\"bus_stop\"]\n",
    "(42.291298,-71.19126,42.447343,-70.971799); \n",
    "out center;\n",
    "\"\"\"\n",
    "response = requests.get(overpass_url, params={'data': overpass_query})\n",
    "data = response.json()\n",
    "\n",
    "stop_list = data.get('elements')\n",
    "print (\"Total number of bus stops: \",len(stop_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (stop_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary to dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for stop in stop_list:\n",
    "    df = df.append(pd.DataFrame.from_dict(stop, orient='columns'))[['id','lat','lon','tags']]\n",
    "    \n",
    "df = df.reset_index()\n",
    "df = df[df['index']=='highway'][['id','lat','lon','tags']].reset_index(drop=True)\n",
    "\n",
    "# Collect coords into list\n",
    "coords = []\n",
    "\n",
    "for element in data['elements']:\n",
    "    if element['type'] == 'node':\n",
    "        lon = element['lon']\n",
    "        lat = element['lat']\n",
    "        coords.append((lon, lat))\n",
    "    elif 'center' in element:\n",
    "        lon = element['center']['lon']\n",
    "        lat = element['center']['lat']\n",
    "        coords.append((lon, lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: it is not always necessary to create this DataFrame.\n",
    "# For example, if you plan to visualize data in D3.js, \n",
    "# you may want to maintain its json format or convert dataframe to json.\n",
    "# (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_json.html)\n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we use matplotlib to visualize all bus stops based on their coordinates:\n",
    "# We will look into matplotlib more in a later workshop session on plotting data in python.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\\\n",
    "\n",
    "X = np.array(coords)# Convert coordinates into numpy array\n",
    "\n",
    "plt.plot(X[:, 0], X[:, 1], 'o', markersize=1)\n",
    "plt.title('Bus Stops in Boston Area')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many city open data portals provide API, so you can access data beyond direct downloading. In addition, here are some popular APIs that are relevant to urban science, happy coding!\n",
    "\n",
    "[Zillow](https://www.zillow.com/howto/api/APIOverview.htm)\n",
    "\n",
    "[Uber](https://developer.uber.com/)\n",
    "\n",
    "[Airbnb](https://www.airbnb.com/partner)\n",
    "\n",
    "[Foursquare](https://developer.foursquare.com/)\n",
    "\n",
    "[Yelp](https://www.yelp.com/developers)\n",
    "\n",
    "[Education Data Explorer by Urban Institute](https://educationdata.urban.org/documentation/#about_the_api)\n",
    "\n",
    "[Bing Maps API](https://www.microsoft.com/en-us/maps/choose-your-bing-maps-api)\n",
    "\n",
    "[Boston MBTA API](https://www.mbta.com/developers/v3-api)\n",
    "\n",
    "[Coord by Sidewalk Labs](https://www.coord.com/api-developers)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
